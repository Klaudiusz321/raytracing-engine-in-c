---
description: 
globs: 
alwaysApply: true
---
Real-time simulation of relativistic effects is computationally demanding. Our architecture incorporates several performance optimizations and best practices:
GPU Acceleration: Offload as much as possible to the GPU. The gravitational lensing (ray bending) is done in shader code on the GPU, which is massively parallel and the only feasible way to compute per-pixel light bending at 60fps. Prior work shows this approach yields “physically accurate gravitational lensing” with decent frame rates​
OSEISKAR.GITHUB.IO
. We will write optimized GLSL code for the fragment shader, possibly using tricks like precomputed lookup tables for the metric potentials or adaptive step sizes to minimize the number of steps per ray​
OSEISKAR.GITHUB.IO
. We will also use GPU instancing to draw many particles if the accretion disk uses particle sprites, which allows the GPU to handle thousands of particles with one draw call.
Level of Detail & Quality Settings: Provide multiple quality levels (e.g. Low, Medium, High, Ultra) that adjust internal parameters:
Low: lower resolution rendering or fewer rays (maybe render at half resolution and upscale), simpler shader (perhaps don’t do Doppler coloring), and fewer particles.
High: full resolution, full effects, more integration steps for accuracy.
Users can switch based on their hardware. We can also dynamically adjust quality – if frame rate drops, reduce quality until it stabilizes. Since this is an educational tool, maintaining interactivity is more important than maximum fidelity at all times.
Adaptive Time Stepping: In high gravity or high-speed situations, physics requires small time steps for accuracy. Instead of using a very small fixed step everywhere (which wastes time in weak-field regions), the physics engine can adapt the step: e.g., use smaller steps when a particle is near the BH or during close approach, and larger steps when all action is far out. This ensures accuracy where needed without uniformly slowing down the simulation. The Leapfrog integrator used in some GPU implementations​
OSEISKAR.GITHUB.IO
 could be adapted for variable step if careful.
Parallelism: Utilize multi-core CPUs by distributing independent tasks. The orbits of many particles around a BH are largely independent (no inter-particle forces if we ignore self-gravity of disk), so each particle’s update can run in parallel. We can use an OpenMP for loop or task-based parallelism in C++ to update all particles each frame concurrently. For web (WASM), threading is a bit more complex (requiring web workers and SharedArrayBuffer), but with proper setup, we could enable it for browsers that support WASM threads. Alternatively, since the heavy per-pixel work is already on GPU, the CPU might be fine single-thread for the rest.
Memory and Data Efficiency: We will minimize data transfer between CPU and GPU. For example, if the lensing shader runs entirely on GPU, we don’t have to copy its results back to CPU (we directly display it). We only send small uniform updates (like camera position, BH mass) to the GPU each frame. Similarly, on the web with WASM, we avoid copying large arrays frequently between JS and WASM – if needed, we can memory-map data so JS can directly read a WASM array. Efficient binary protocols or struct layouts will be used to avoid overhead in serialization/deserialization on each frame.
Caching and Precomputation: Identify any computations that can be done ahead of time. For instance, if simulating a non-rotating black hole, the bending angle as a function of impact parameter could be precomputed and stored, but since our approach is more general, we might not use that. However, for something like a static accretion disk texture (the appearance of the disk under gravity if viewed face-on might be pregenerated), we could generate that once and reuse it. If the user frequently toggles a certain scenario, we could cache the last known particle positions, etc. (Though our focus is more on real-time, not repeated runs efficiency.)
Profiling and Tuning: As an open-source project, we will include profiling instrumentation (especially in the physics engine) to measure performance of each subsystem. This will guide further optimization. For example, if we find the geodesic integrator is the bottleneck, we might try a different numerical method or move more of it to GPU. If drawing the disk is the bottleneck, we might reduce particle count or switch to a shader-based disk.
Fallbacks: Ensure the system fails gracefully on lower-end hardware. If a user’s machine cannot handle the full simulation, we could fallback to a simpler demonstration (for instance, show a pre-rendered video or a simplified shader that uses an approximation of lensing). The architecture allows disabling modules: e.g. turn off the live accretion disk simulation and just show a static image of a disk for very slow machines. This way the educational message still comes across even on weaker devices.
Figure: Simulated view of a black hole with a glowing accretion disk (rendering by Bronzwaer et al.) — our demo will emulate such visuals in real time.
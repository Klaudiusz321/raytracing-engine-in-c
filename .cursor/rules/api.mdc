---
description: 
globs: 
alwaysApply: true
---
To connect the frontend UI with the backend engine, especially in a web deployment where the engine might run on a server or in a WebWorker, we design a flexible API layer. This layer can operate in two modes:
Local Mode (WebAssembly or In-App): In the web app, if we compile the physics engine to WebAssembly and run it in the browser, the ‚ÄúAPI‚Äù calls are just function calls in the same context. Similarly, in the desktop app, the UI calls the engine‚Äôs functions directly. In these cases, the communication is via in-memory function calls ‚Äì very fast and with no networking needed. We still logically separate the API (to keep the possibility of remote mode), but it might be a thin wrapper around direct calls.
Client-Server Mode (Remote): We may run the physics engine on a remote server (or a localhost server) in cases where the browser can‚Äôt handle the computation or to allow multiple clients to connect to a single simulation (collaborative scenario, or one heavy simulation feeding many viewers). In this mode, the frontend communicates over HTTP/WebSocket to a backend service. Communication Protocol: For real-time interaction, WebSockets are preferred because they allow continuous two-way communication with low latency‚Äã
reddit.com
‚Äã
ably.com
. The client can send a message like {"type": "set_param", "mass": 8.0} and the server responds with acknowledgments or state updates. The server can push messages like {"type": "frame_update", "particles": [...], "time": 123.4} at regular intervals (say, 30 times per second) to update the client with new simulation data. This avoids the overhead of repeated HTTP requests and enables streaming data flows which are vital for smooth real-time updates‚Äã
reddit.com
. We will design a simple message schema in JSON for clarity (or use a binary format for efficiency if needed). For example:
Client to server messages: set_param, toggle_effect, reset, request_state, etc.
Server to client: state_update (with positions, etc.), ack (confirming a param set), or even event (for special events like ‚Äúparticle fell into BH‚Äù if we choose to notify those).
We will still incorporate a small REST API for convenience on certain actions ‚Äì e.g., to fetch static information like a list of preset scenarios or to load a specific scenario configuration via a URL. REST is well-suited for one-off requests or initial data loading, while WebSockets handle the live stream of simulation frames and interactive commands‚Äã
ably.com
.
Python Server Implementation: We plan to implement the server in Python (if using a remote server) for ease of development and integration with scientific libraries. We can use FastAPI or Flask for REST endpoints and WebSocket (asyncio) for the real-time channel. The Python server will load the C++ physics engine through Python bindings. For example, we might have a module pyBlackHole that wraps the C++ engine. When a WebSocket message arrives (say set_param), the server calls the corresponding pyBlackHole.set_param() function. The physics engine could be running in a loop in a background thread, or step-by-step when triggered by the server (depending on design). A possible design is the server runs the physics in realtime loop and broadcasts state updates to clients continuously. Alternatively, the server advances the simulation only when a client is connected and requesting frames. We will ensure thread-safety ‚Äì the engine may run in one thread while the networking runs in another, using locks or concurrent queues to exchange data.
Data to Frontend: The main data needing transmission each frame might be relatively small: positions of a few objects, maybe a texture or image if we ever send pre-rendered data (but likely we render on the client). We specifically avoid sending raw pixel data over WebSocket per frame (that would essentially be streaming a video, which is inefficient compared to rendering via WebGL on client). Instead, the server sends simulation parameters and object states, and the client‚Äôs GPU does the heavy rendering. For instance, the server might send the current transformation matrix of an orbiting star, and the client moves the star in the WebGL scene accordingly; the gravitational lensing shader on the client already knows how to draw the background with lensing given the black hole parameter set.
WebAssembly Alternative for Communication: When running entirely in-browser with WebAssembly, we don‚Äôt need sockets. Communication is through function calls or shared memory. For instance, the JS UI can call a WASM-exported function simulateOneStep(dt) and then read a WASM memory buffer that contains particle coordinates to update Three.js objects. This is extremely low-latency (just a memory copy). We will hide this behind a common interface so the frontend code doesn‚Äôt care if data came from a WebSocket message or a WASM memory ‚Äì it will just receive an update and apply it.
API Documentation & Openness: Since this project is open-source and intended for educational use, we will document the API so that others can write their own frontends or analysis scripts. For example, a researcher could use the Python API to run the simulation headless and gather data on light paths, or a developer could create an alternate UI (maybe a mobile app) that uses our WebSocket protocol to drive the simulation remotely. We will publish the message formats, parameter definitions (mass units, etc.), and any important conventions (like coordinate systems used: probably we use a simple Cartesian coordinate system centered on the black hole, with units where 
ùê∫
=
ùëê
=
1
G=c=1 for convenience‚Äã
oseiskar.github.io
, and we‚Äôll document how those translate to physical units).
---
description: 
globs: 
alwaysApply: true
---
Graphics API Abstraction: For broad compatibility, we can initially use OpenGL (likely OpenGL 4.x or OpenGL ES 3.0 for parity with WebGL 2). OpenGL can render the scene and run GLSL shaders analogous to the WebGL shader for ray tracing. In the future, or as an alternative path, we can implement a Vulkan renderer to take advantage of modern GPU features and possibly ray-tracing extensions. The architecture could use an abstraction layer or engine (such as Magnum or bgfx) that supports multiple backends (OpenGL, Vulkan, WebGL) under the hood, so we write rendering code once and deploy it on different platforms. This ensures that the same shader logic for gravitational lensing can be reused. If using an engine or framework is undesirable, we can maintain separate code paths (GLSL shader code can be shared between WebGL and desktop GL, and a Vulkan compute shader version can be written for experimentation with ray-tracing pipelines).
Rendering and Shaders: The desktop app will render the black hole, sky, accretion disk, etc. using the native GPU. We will reuse the shader algorithms from the web (possibly with higher precision or more steps since desktops can handle more). For example, a fragment shader in OpenGL could integrate photon paths in double precision (if supported) for higher accuracy lensing, or we might implement a compute shader that performs a screen-space raymarching for the BH and writes to a texture. The accretion disk might be drawn as a collection of particles or a textured quad ring that’s shaded with relativistic effects (e.g. apply a Doppler boost to brightness on one side). Desktop GPUs also allow using libraries like OpenCL or CUDA if needed for physics (though we aim to stick to cross-platform solutions like OpenGL/Vulkan).
User Interface: For the desktop UI controls, we can use libraries such as ImGui (Dear ImGui) for an in-app GUI overlay, or a more full-featured toolkit (Qt, GTK) if we want a windowed interface. ImGui is lightweight and easy to integrate with OpenGL/Vulkan, perfect for developer-facing tools or demos. It can create sliders, checkboxes, etc., similar to the web’s dat.GUI. We will mirror the control set from the web version: parameter sliders for mass, spin, etc., toggles for effects, and options to change camera mode. Because a desktop app isn’t constrained by browser sandbox, we could also allow deeper profiling or debugging UI (like a graph of frame time, or a switch to enable a wireframe overlay for debugging the accretion disk mesh).
Integration of Physics Engine: On desktop, the C physics engine can be linked directly (as a static or dynamic library) or compiled in. The rendering loop will call into the physics engine each frame (or as needed) to update simulation state. For instance, each frame the app might do: physicsEngine.stepSimulation(timeStep), then fetch the updated positions of particles or any other data needed for rendering, then issue draw calls. Because this happens in the same process, latency is minimal. We can take advantage of multi-threading: the physics engine could run in a separate thread or utilize multiple cores (especially if simulating many particles in the accretion disk or doing CPU ray computations for any reason). The engine’s use of parallelism (via std::thread, TBB, or OpenMP) will be configured to not interfere with the rendering thread (perhaps by double-buffering data or using thread-safe queues for passing updates).
Advanced Features on Desktop: The desktop environment allows us to push fidelity further. For example, we could experiment with a Kerr metric ray tracer (including frame-dragging effects of a spinning black hole) since more computation can be done on a desktop GPU/CPU. We might integrate optional modules like a general relativistic magneto-hydrodynamics (GRMHD) data loader to visualize prerecorded simulation data for the accretion disk (if we have data from a physics simulation, we could render it in our framework). Another possibility is VR or 360-degree video output: the desktop version could render stereo views for VR headsets, given its greater control over hardware. These are future extensions; the architecture leaves room for them by keeping rendering modular (so we could swap the camera to a VR camera, or stream the render to a 360 video).
Cross-Platform Deployment: The desktop app will be developed with cross-platform libraries (for windowing we can use GLFW or SDL2, which work on Windows, Mac, Linux). The build system (CMake for example) will be configured to build on all major OSes. We will also ensure the code uses portable libraries for any system tasks (for example, use C++11 <thread> for threading, which is cross-platform, and avoid OS-specific APIs). Testing will be done on multiple platforms to iron out any differences (especially Mac, which might have OpenGL quirks or prefer Metal – in future we might use MoltenVK for Vulkan on Mac).